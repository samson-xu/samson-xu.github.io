---
title: Hadoop在生信方面的应用
date: 2017-07-05 10:33:47
tags:
- Hadoop
categories:
- Hadoop
comments: ture
---

# <center>Hadoop简介
&emsp;&emsp;首先谈谈我对Hadoop技术的理解。Hadoop是使用Java开发的以HDFS和MapReduce为核心引擎的大数据分布式处理框架。其中，HDFS是一个易于扩展的分布式文件系统，由于自身提供容错机制、数据存在多个副本，不仅可以部署在普通廉价机器上，而且数据可靠性还得到了保障。MapReduce通过Map（映射）和Reduce（化简）来实现大规模数据（TB级）的并行计算。打个比方来说，MapReduce的运作方式就像快递公司一样。物流部门会将发往各地的包裹先运送到各地的物流分站，再由分站派出进行派送；快递员等每个包裹的用户签单后将数据反馈给系统汇总，最终完成整个快递流程。当然，Hadoop还包含了许多其它的组件，例如Hive，Hbase,Pig等等。以Pig为例，由于MapReduce的一个主要的缺点就是开发周期太长，Pig做为它的补充，在大型数据集的处理上提供了更高层次的抽象，它仅仅只需要几行Pig Latin代码就能处理TB级别的数据，极大的提高了我们的工作效率。
# <center>生信应用
&emsp;&emsp;大数据的分析有很多种方法和工具，例如传统的高性能计算，网格计算。在高性能计算中，计算与存储分开，计算节点通过SAN网络访问数据，由于存储设备网络带宽限制，只适合计算密集型作业。此外MPI分布式编程对于程序员拥有更高的要求，远不如Hadoop高度抽象的map，reduce操作。网格计算通过网络将不同的计算机联系起来，性能不如高速网络连接在单一数据中心内的Hadoop集群。
&emsp;&emsp;下面以BWA、GATK为例，谈一谈Hadoop的应用。我们都知道，个人基因组分析目前使用最为广泛的工具就是BWA，GATK。由于个人基因组数据量大，分析效率较低，我们可以将其抽象成MapReduce模型，进行高度并行运算。具体一点来说就是，首先将同一个样品的测序reads分成很多份，分别比对到参考基因组上，接着将BAM合并排序，再将BAM文件按染色体拆分，然后按染色体使用GATK Call variant，最后合并成最终的VCF文件。总之，其核心思想就是不断切分，高度并行化，以计算资源换取时间，从而提高效率。

# <center>总结
&emsp;&emsp;在我看来，生物医学大数据主要包括，以高通量测序为代表的生命组学数据，以靶向药物研发为代表的药物研究实验产生的过程数据，以电子病历为代表的临床医疗服务数据，以居民电子健康档案为代表的个人健康监测与健康管理数据，以及以疾病监测和卫生监督为代表的公共卫生管理数据。现阶段，我们主要研究的还是生命组学数据，在其它方向上还大有可为。
# <center>展望
&emsp;&emsp;最后，我想再探讨一下技术。Hadoop主要是用于大容量静态数据的批处理操作，所以比较适合计算密集型和拥有海量数据的生命组学。Spark是建立在HDFS的基础上，既适用于批处理，也适用于流处理的混合架构框架。它在Hadoop的基础上进行了一些改良，与Hadoop最大的不同点在于，Hadoop使用硬盘来存储数据，而Spark使用内存来存储数据，因此Spark可以提供超过Hadoop 100倍的运算速度。但是，由于内存断电后会丢失数据，Spark不能用于处理需要长期保存的数据。因此，Hadoop常用于离线的复杂的大数据处理，Spark常用于离线的快速的大数据处理。